        - const data_path = "nytimes.dat.perm.5K"
        - const vocab_size = 110000
        - const num_topics = 100
        - const alpha = 0.1
        - const beta = 0.1
        - const alpha_beta = alpha * beta
        - const num_iterations = 2
        -
        - num_tokens = 0
        - num_dist_tokens = 0
        - num_docs = 0
        -
        - function parse_line(line_num::Int64, line::AbstractString)::Vector{Tuple{Tuple{Int64, Int64, Int64},
        -                                                                       Int32}
        -                                                                 }
        -     # Parse each line to be word counts
 78315682     tokens = split(strip(line),  ' ')
   400000     token_vec = Vector{Tuple{Tuple{Int64, Int64, Int64},
        -                             Int32}
        -                       }()
        0     idx = 1
    71824     global num_docs += 1
        0     for token in tokens[2:end]
224597184         word_count = split(token, ":")
        0         word_id = parse(Int64, word_count[1]) + 1
        0         count = parse(Int32, word_count[2])
 18710832         global num_tokens += count
 18708256         global num_dist_tokens += 1
        0         for c = 1:count
155334736             push!(token_vec, ((line_num, word_id, c), 0))
        -         end
        0         idx += 1
        -     end
        0     return token_vec
        - end
        -
        - function load_data(path::AbstractString)::Vector{Tuple{Tuple{Int64, Int64, Int64},
        -                                                        Int32}
        -                                                  }
  6002784     docs = Vector{Tuple{Tuple{Int64, Int64, Int64}, Int32}}()
        0     num_lines::Int64 = 1
       32     open(path) do dataf
 10434423         for line in eachline(dataf)
        0             doc = parse_line(num_lines, line)
 20393280             append!(docs, doc)
    71840             num_lines += 1
        -         end
        -     end
        0     return docs
        - end
        -
        - println("loading data")
        - topic_assignments = load_data(data_path)
        -
        - println("num_tokens = ", num_tokens)
        - println("num_dist_tokens = ", num_dist_tokens)
        - println("num_docs is ", num_docs)
        -
        - topic_summary = zeros(Int64, num_topics)
        - topic_summary_buff = zeros(Int64, num_topics)
        - word_topic_table = [Dict{Int32, UInt64}() for w = 1:vocab_size]
        - doc_topic_table = [Dict{Int32, UInt64}() for d = 1:num_docs]
        - doc_topic_assignmnts = Vector{Vector{Int32}}()
        -
        - srand(1)
        - println("initialization")
        - for idx in eachindex(topic_assignments)
238975047     doc_id = topic_assignments[idx][1][1]
241586000     word_id = topic_assignments[idx][1][2]
     7185     word_topic_dict = word_topic_table[word_id]
        0     doc_topic_dict = doc_topic_table[doc_id]
        -
        0     topic = rand(1:num_topics)
295505414     topic_assignments[idx] = (topic_assignments[idx][1], topic)
   202167     word_topic_count = get(word_topic_dict, topic, 0)
 39308495     word_topic_dict[topic] = word_topic_count + 1
        0     doc_topic_count = get(doc_topic_dict, topic, 0)
 24496012     doc_topic_dict[topic] = doc_topic_count + 1
 52088000     topic_summary[topic] += 1
        - end
        -
        - println("initialization done")
        -
        - function create_word_topic_vec(word_topic_dict::Dict{Int32, UInt64}
        -                                )::Tuple{Vector{UInt64}, Vector{Int32}}
  9237348     num_nonzero_counts = length(word_topic_dict)
 15410560     word_topic_count_vec = Vector{UInt64}(num_nonzero_counts)
 15410560     word_topic_index_vec = Vector{Tuple{Int32, Int32}}(num_nonzero_counts)
        0     index = 1
        0     for (topic, topic_count) in word_topic_dict
        0         word_topic_count_vec[index] = topic_count
        0         word_topic_index_vec[index] = (topic, index)
        0         index += 1
        -     end
        0     @assert index == length(word_topic_index_vec) + 1
 29514458     sort!(word_topic_index_vec, by = x -> word_topic_count_vec[x[2]], rev = true)
 11464381     sort!(word_topic_count_vec, rev = true)
 14249712     word_topic_vec = map(x -> x[1], word_topic_index_vec)
  3520000     return (word_topic_count_vec, word_topic_vec)
        - end
        -
        - word_topic_vec_table = map(create_word_topic_vec, word_topic_table)
        - word_topic_table = Vector{Dict{Int32, UInt64}}()
        -
        - println("constructed word_topic_vec_table")
        - const beta_sum = beta * vocab_size
        -
        - s_sum = 0.0
        - s_sum_buff = 0.0
        - for topic_count in topic_summary
     4800     s_sum += alpha_beta / (beta_sum + topic_count)
        - end
        -
        - println(s_sum)
        -
        - r_sum = zeros(num_docs)
        - for doc_id in eachindex(doc_topic_table)
        0     doc_topic_dict = doc_topic_table[doc_id]
        0     doc_r_sum = 0.0
 29292960     for topic_count in doc_topic_dict
 14643398         topic = topic_count[1]
 21952656         count = topic_count[2]
 36710070         doc_r_sum += count * beta / (beta_sum + topic_summary[topic])
        -     end
        0     r_sum[doc_id] = doc_r_sum
        - end
        -
        - q_coeff = [zeros(Float32, num_topics) for i = 1:num_docs]
        -
        - for doc_id in eachindex(doc_topic_table)
        0     doc_topic_dict = doc_topic_table[doc_id]
     7185     q_coeff_vec = q_coeff[doc_id]
        0     for topic = 1:num_topics
        0         count = get(doc_topic_dict, topic, 0)
 32053686         q_coeff_vec[topic] = (alpha + count) / (beta_sum + topic_summary[topic])
        -     end
      944     q_coeff[doc_id] = q_coeff_vec
        - end
        -
        - word_log_gamma_sum = zeros(num_topics)
        - llh = 0.0
        -
        - function find_index(vec::Vector{Int32},
        -                     key)
     5072     for idx in eachindex(vec)
        0         element = vec[idx]
        0         if key == element
        0             return idx
        -         end
        -     end
        0     return -1
        - end
        -
        -
        - llh_vec = Vector{Float64}()
        - word_llh_vec = Vector{Float64}()
        -
        - time_vec = Vector{Float64}()
        - start_time = now()
        -
        - function sample_one_word(topic_assignment,
        -                          doc_topic_table,
        -                          word_topic_vec_table,
        -                          topic_summary,
        -                          s_sum,
        -                          r_sum,
        -                          q_coeff,
        -                          topic_summary_buff,
        -                          s_sum_buff,
        -                          alpha,
        -                          alpha_beta,
        -                          beta,
        -                          num_topics,
        -                          beta_sum,
        -                          q_term_val,
        -                          q_term_topic)
        -
485602375     doc_id = topic_assignment[1][1]
        0     word_id = topic_assignment[1][2]
        0     old_topic = topic_assignment[2]
        -
        0     doc_topic_dict = doc_topic_table[doc_id]
        0     word_topic_vec_pair = word_topic_vec_table[word_id]
        0     word_topic_vec = word_topic_vec_pair[2]
        0     word_topic_count_vec = word_topic_vec_pair[1]
        0     doc_q_coeff = q_coeff[doc_id]
        -
        0     doc_old_topic_count = doc_topic_dict[old_topic]
        -
        0     q_sum = 0.0
        0     num_nonzero_q_terms = 0
        0     for index in eachindex(word_topic_vec)
        0         topic_count = word_topic_count_vec[index]
        0         topic = word_topic_vec[index]
        0         q_term = doc_q_coeff[topic] * topic_count
        0         num_nonzero_q_terms += 1
        0         q_term_val[num_nonzero_q_terms] = q_term
        0         q_term_topic[num_nonzero_q_terms] = topic
        0         q_sum += q_term
        -     end
        -
        0     @assert q_sum >= 0
        0     denom = topic_summary[old_topic] + beta_sum
        0     s_sum_buff -= alpha_beta / denom
        0     s_sum_buff += alpha_beta / (denom - 1)
        0     r_sum[doc_id] -= (doc_old_topic_count * beta) / denom
        0     r_sum[doc_id] += ((doc_old_topic_count - 1) * beta) / (denom - 1)
        0     doc_q_coeff[old_topic] = (alpha + doc_old_topic_count - 1) / (denom - 1)
        0     doc_r_sum = r_sum[doc_id]
        0     @assert old_topic in keys(doc_topic_dict)
        0     doc_topic_dict[old_topic] -= 1
        0     if doc_topic_dict[old_topic] == 0
        0         delete!(doc_topic_dict, old_topic)
        -     end
        -
        0     old_topic_index = find_index(word_topic_vec, old_topic)
        0     old_topic_count = word_topic_count_vec[old_topic_index]
        0     word_topic_count_vec[old_topic_index] = old_topic_count - 1
        0     q_sum -= q_term_val[old_topic_index]
        0     q_term_val[old_topic_index] = (old_topic_count - 1) * doc_q_coeff[old_topic]
        0     q_sum += q_term_val[old_topic_index]
        -
        0     topic_summary_buff[old_topic] -= 1
        0     total_mass = q_sum + doc_r_sum + s_sum
        0     sample = rand() * total_mass
        -
        0     new_topic = Int32(0)
        0     if sample < q_sum
        0         for idx in 1:num_nonzero_q_terms
        0             sample -= q_term_val[idx]
        0             if sample < 0
        0                 new_topic = q_term_topic[idx]
        0                 break
        -             end
        -         end
        0         if sample >= 0
        0             new_topic = q_term_topic[num_nonzero_q_terms]
        -         end
        -
        0     elseif sample < q_sum + doc_r_sum
        0         sample -= q_sum
        0         sample /= beta
        0         topic_last = 1
        0         for (topic, topic_count) in doc_topic_dict
        0             sample -= topic_count / (topic_summary[topic] + beta_sum)
        0             topic_last = topic
        0             if sample < 0
        0                 new_topic = topic
        0                 break
        -             end
        -         end
        0         if sample >= 0
        0             new_topic = topic_last
        -         end
        0     elseif sample <= q_sum + doc_r_sum + s_sum
        0         sample -= q_sum + doc_r_sum
        0         sample /= alpha_beta
        0         for t in 1:num_topics
        0             sample -= 1.0 / (topic_summary[t] + beta_sum)
        0             if sample < 0
        0                 new_topic = t
        0                 break
        -             end
        -         end
        0         if sample >= 0
        0             new_topic = Int32(num_topics)
        -         end
        -     else
        0         error("sample = ", sample, " total_mass = ", total_mass)
        -     end
        -
 53724800     denom = topic_summary[new_topic] + beta_sum
        0     s_sum_buff -= alpha_beta / denom
        0     s_sum_buff += alpha_beta / (denom + 1)
 53727552     if new_topic in keys(doc_topic_dict)
     4160         r_sum[doc_id] -= (doc_topic_dict[new_topic] * beta) / denom
        0         r_sum[doc_id] += ((doc_topic_dict[new_topic] + 1) * beta) / (denom + 1)
 53299936         doc_q_coeff[new_topic] = (alpha + doc_topic_dict[new_topic] + 1) / (denom + 1)
        0         doc_topic_dict[new_topic] += 1
        -     else
        0         r_sum[doc_id] += beta / denom
   426176         doc_q_coeff[new_topic] = (alpha + 1) / (denom + 1)
     2944         doc_topic_dict[new_topic] = 1
        -     end
        0     @assert r_sum[doc_id] >= 0
        0     new_topic_index = find_index(word_topic_vec, new_topic)
        0     if new_topic_index != -1
        0         new_topic_count = word_topic_count_vec[new_topic_index]
        0         word_topic_count_vec[new_topic_index] = new_topic_count + 1
        0         q_sum -= q_term_val[new_topic_index]
        -     else
        0         new_topic_count = 1
  5998512         push!(word_topic_vec, new_topic)
 11188192         push!(word_topic_count_vec, 1)
        0         num_nonzero_q_terms += 1
        0         new_topic_index = length(word_topic_vec)
        -     end
161178304     q_term_val[new_topic_index] = new_topic_count * doc_q_coeff[new_topic]
        0     q_sum += q_term_val[new_topic_index]
 16081664     topic_summary_buff[new_topic] += 1
        -
        0     return new_topic
        - end
        -
        - function sample_all_words(topic_assignments,
        -                           doc_topic_table,
        -                           word_topic_vec_table,
        -                           topic_summary,
        -                           s_sum,
        -                           r_sum,
        -                           q_coeff,
        -                           topic_summary_buff,
        -                           s_sum_buff,
        -                           alpha,
        -                           alpha_beta,
        -                           beta,
        -                           num_topics,
        -                           beta_sum,
        -                           q_term_val,
        -                           q_term_topic,
        -                           iteration)
   115274     for idx in eachindex(topic_assignments)
        0         topic_assignment = topic_assignments[idx]
        0         if iteration > 1
        0             new_topic = sample_one_word(topic_assignment,
        -                                         doc_topic_table,
        -                                         word_topic_vec_table,
        -                                         topic_summary,
        -                                         s_sum,
        -                                         r_sum,
        -                                         q_coeff,
        -                                         topic_summary_buff,
        -                                         s_sum_buff,
        -                                         alpha,
        -                                         alpha_beta,
        -                                         beta,
        -                                         num_topics,
        -                                         beta_sum,
        -                                         q_term_val,
        -                                         q_term_topic)
        -         else
        0             new_topic = sample_one_word(topic_assignment,
        -                                         doc_topic_table,
        -                                         word_topic_vec_table,
        -                                         topic_summary,
        -                                         s_sum,
        -                                         r_sum,
        -                                         q_coeff,
        -                                         topic_summary_buff,
        -                                         s_sum_buff,
        -                                         alpha,
        -                                         alpha_beta,
        -                                         beta,
        -                                         num_topics,
        -                                         beta_sum,
        -                                         q_term_val,
        -                                         q_term_topic)
        -         end
429808512         topic_assignments[idx] = (topic_assignment[1], new_topic)
        -     end
        - end
        -
        - function train(topic_assignments,
        -                doc_topic_table,
        -                word_topic_vec_table,
        -                topic_summary,
        -                s_sum,
        -                r_sum,
        -                q_coeff,
        -                topic_summary_buff,
        -                s_sum_buff,
        -                alpha,
        -                alpha_beta,
        -                beta,
        -                num_topics,
        -                beta_sum)
 34754519     last_time = start_time
      544     q_terms_val = Vector{Float32}(num_topics)
      544     q_terms_topic = Vector{Int32}(num_topics)
        0     for iteration = 1:num_iterations
     1248         println("iteration = ", iteration)
        0         @time sample_all_words(topic_assignments,
        -                                doc_topic_table,
        -                                word_topic_vec_table,
        -                                topic_summary,
        -                                s_sum,
        -                                r_sum,
        -                                q_coeff,
        -                                topic_summary_buff,
        -                                s_sum_buff,
        -                                alpha,
        -                                alpha_beta,
        -                                beta,
        -                                num_topics,
        -                                beta_sum,
        -                                q_terms_val,
        -                                q_terms_topic,
        -                                iteration)
        -
        -
        0         topic_summary .+= topic_summary_buff
     1792         topic_summary_buff = zeros(Int64, num_topics)
        0         s_sum += s_sum_buff
        0         s_sum_buff = 0.0
        -
  3520000         for word_topic_vec_idx in eachindex(word_topic_vec_table)
        0             word_topic_count_vec = word_topic_vec_table[word_topic_vec_idx][1]
        0             word_topic_vec = word_topic_vec_table[word_topic_vec_idx][2]
 31480528             word_topic_index_vec = Vector{Tuple{Int32, Int32}}(length(word_topic_vec))
        0             @assert length(word_topic_count_vec) == length(word_topic_vec)
        0             for idx in eachindex(word_topic_vec)
        0                 word_topic_index_vec[idx] = (word_topic_vec[idx], idx)
        -             end
 57637221             sort!(word_topic_index_vec, by = x -> word_topic_count_vec[x[2]], rev = true)
 21120000             sort!(word_topic_count_vec, rev = true)
 28666784             word_topic_vec = map(x -> x[1], word_topic_index_vec)
        -
        0             num_nonzeros = 0
 51728176             for topic_count in word_topic_count_vec
        0                 if topic_count == 0
        0                     break
        -                 end
        0                 num_nonzeros += 1
        -             end
        0             resize!(word_topic_count_vec, num_nonzeros)
        0             resize!(word_topic_vec, num_nonzeros)
        0             @assert length(word_topic_count_vec) == length(word_topic_vec)
  7041664             word_topic_vec_table[word_topic_vec_idx] = (word_topic_count_vec, word_topic_vec)
        -         end
        0         llh = 0
        -
        0         if iteration % 1 == 0 ||
        -             iteration == num_iterations
        0             for word_topic_vec_index in eachindex(word_topic_vec_table)
        0                 word_topic_vec_pair = word_topic_vec_table[word_topic_vec_index]
        0                 word_topic_count_vec = word_topic_vec_pair[1]
        0                 word_topic_vec = word_topic_vec_pair[2]
        -
        0                 @assert length(word_topic_count_vec) == length(word_topic_vec)
        0                 for idx in eachindex(word_topic_vec)
        0                     topic = word_topic_vec[idx]
        0                     count = word_topic_count_vec[idx]
 45855978                     word_log_gamma_sum[topic] += lgamma(count + beta)
        -                 end
 10560976                 llh += (num_topics - length(word_topic_vec)) * lgamma(beta)
        -             end
     6432             for topic in eachindex(word_log_gamma_sum)
     3200                 topic_log_gamma_sum_val = word_log_gamma_sum[topic]
    22176                 llh += topic_log_gamma_sum_val - lgamma(vocab_size * beta + topic_summary[topic])
     6400                 llh += lgamma(vocab_size * beta) - vocab_size * lgamma(beta)
        0                 word_log_gamma_sum[topic] = 0.0
        -             end
        0             word_llh = llh
    72728             push!(word_llh_vec, word_llh)
        -             # compute topic likelihood
        0             for doc_topic_dict in doc_topic_table
        0                 doc_log_gamma_sum = 0.0
        0                 doc_total_word_count = 0
        0                 for topic = 1:num_topics
        0                     count = get(doc_topic_dict, topic, 0)
    56832                     doc_total_word_count += count
 32000976                     doc_log_gamma_sum += lgamma(count + alpha)
        -                 end
   641376                 llh += doc_log_gamma_sum - lgamma(alpha * num_topics + doc_total_word_count)
   320000                 llh += lgamma(num_topics * alpha) - num_topics * lgamma(alpha)
        -             end
       48             push!(llh_vec, llh)
      256             curr_time = now()
    73118             diff_time = Int(Dates.value(curr_time - last_time)) / 1000
       32             last_time = curr_time
      128             elapsed = Int(Dates.value(curr_time - start_time)) / 1000
       80             push!(time_vec, elapsed)
   106248             println("iteration = ", iteration, " elapsed = ", elapsed, " llh = ", llh, " word_llh = ", word_llh)
        -         end
        -     end
        - end
        -
        - train(topic_assignments,
        -       doc_topic_table,
        -       word_topic_vec_table,
        -       topic_summary,
        -       s_sum,
        -       r_sum,
        -       q_coeff,
        -       topic_summary_buff,
        -       s_sum_buff,
        -       alpha,
        -       alpha_beta,
        -       beta,
        -       num_topics,
        -       beta_sum)
        -
        - println(time_vec)
        - println(word_llh_vec)
        - println(llh_vec)
        -
